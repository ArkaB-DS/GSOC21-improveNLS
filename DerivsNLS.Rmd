---
title: "Jacobian Calculations for nls()"
author:
  - Arkajyoti Bhattacharjee, Indian Institute of Technology, Kanpur
  - John C. Nash, University of Ottawa, Canada
date: "26/05/2021"
output: pdf_document
bibliography: ImproveNLS.bib
---

<!-- - Heather Turner, University of Warwick, UK -->

```{r setup, include=FALSE}
rm(list=ls()) # clear the workspace for this document
knitr::opts_chunk$set(echo = TRUE)
## setup for this document
library(nlsr)  # So we have the analytic/symbolic derivatives
library(numDeriv) # numerical derivatives package
library(microbenchmark)  # timing

printsum <- function(xx){ print(summary(xx))} # May be needed
traceval  <-  TRUE  # traceval set TRUE to debug or give full history
#  Set to FALSE when we don't need extensive output
```

# ISSUES

- ExDerivs.R file causes a number of failures in the ORIGINAL numericDeriv.

- Need to verify nlsalt:: version matches all cases of nlspkg:: version


# Jacobians in nls()

`nls()` needs Jacobians calculated at the current set of trial nonlinear model
parameters to set up the Gauss-Newton equations. Unfortunately, `nls()` calls
the Jacobian the "gradient", and uses function `numericDerivs()` to compute them.
This document is an attempt to describe different ways to compute the Jacobian
for use in nls() and related software, and to evaluate the performance of these
approaches.

In evaluating performance, we need to know the conditions under which the evaluation
was conducted. Thus the computations included in this document, which is built using
`Rmarkdown`, are specific to the computer in which the document is processed. We
will add tables that give the results for different computing environments at the
bottom.

# An example problem

We will use the Hobbs weed infestation problem (@jncnm79, page 120).

```{r hobbsex}
# Data for Hobbs problem
ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 
            38.558, 50.156, 62.948, 75.995, 91.972) # for testing
tdat  <-  seq_along(ydat) # for testing

# A simple starting vector -- must have named parameters for nlxb, nls, wrapnlsr.
start1  <-  c(b1=1, b2=1, b3=1)
eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
str(eunsc)
# Can we convert a string form of this "model" to a formula
ceunsc <- " y ~ b1/(1+b2*exp(-b3*tt))"
str(ceunsc)

# Will be TRUE if we have made the conversion
print(as.formula(ceunsc)==eunsc)

## LOCAL DATA IN DATA FRAMES
weeddata1  <-  data.frame(y=ydat, tt=tdat)

## Put data in an Environment
weedenv <- list2env(weeddata1)
weedenv$b1 <- start1[[1]]
weedenv$b2 <- start1[[2]]
weedenv$b3 <- start1[[3]]
# Display content of the Environment
## Note that may need to do further commands to get everything
ls.str(weedenv)
# Generate the residual "call"
rexpr<-call("-",eunsc[[3]], eunsc[[2]])
# Get the residuals
r0<-eval(rexpr, weedenv)
print(r0)
cat("Sumsquares at 1,1,1 is ",sum(r0^2),"\n")

## Another way
ldata<-list2env(as.list(start1),envir=weedenv)
ldata
ls.str(ldata)
eval(rexpr,envir=ldata)

## Do we need to get a model frame? How? and How to use it?

## Now ready to try things out.
```

# Tools for Jacobians

## numericDeriv() original version

`numericDeriv` is the R function used by `nls()` to evaluate Jacobians for its Gauss-Newton
equations. The R source code is in the file `nls.R`. It calls a C function numeric_deriv
in `nls.c`. 

```{r nd1}
## Seems to work -- BUT note file ExDerivs.R has many "failures"??
theta <- c("b1", "b2", "b3")
ndeunsc<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv)
print(ndeunsc)
print(sum(ndeunsc^2))
tndeunsc<-microbenchmark(ndeunsc<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv))
print(tndeunsc)
## numericDeriv also has central difference option, as well as choice of eps parameter
## Central diff
ndeunsc2<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE)
print(ndeunsc2)
print(sum(ndeunsc2^2))
tndeunsc2<-microbenchmark(ndeunsc2<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE))
print(tndeunsc2)

## Forward diff with smaller eps
ndeunscx<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, eps=1e-10)
print(ndeunscx)
print(sum(ndeunscx^2))
tndeunscx<-microbenchmark(ndeunscx2<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, eps=1e-10))
print(tndeunscx)

## Central diff with smaller eps
ndeunscx2<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE, eps=1e-10)
print(ndeunscx2)
print(sum(ndeunscx2^2))
tndeunscx2<-microbenchmark(ndeunscx2<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE, eps=1e-10))
print(tndeunscx2)

## Add dir parameter
## to forward diff
ndeunsc<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv)
print(ndeunsc)
ndeunscd<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, dir=-1)
print(ndeunscd)
ndeunscc<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE)
print(ndeunscc)
ndeunsccd<-nlspkg::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE, dir=-1)
print(ndeunsccd)

```


## numericDeriv() alternative pure-R version

This version (see Appendix 2) has C code replaced with R equivalents.


```{r and1}
## Try ExDerivs.R ??
andeunsc<-nlsalt::numericDeriv(rexpr, theta, rho=weedenv)
print(andeunsc)
print(sum(andeunsc^2))
tandeunsc<-microbenchmark(andeunsc<-nlsalt::numericDeriv(rexpr, theta, rho=weedenv))
print(tandeunsc)
## numericDeriv also has central difference option, as well as choice of eps parameter
## Central diff
andeunsc2<-nlsalt::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE)
print(andeunsc2)
print(sum(andeunsc2^2))
tandeunsc2<-microbenchmark(andeunsc2<-nlsalt::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE))
print(tandeunsc2)

## Forward diff with smaller eps
andeunscx<-nlsalt::numericDeriv(rexpr, theta, rho=weedenv, eps=1e-10)
print(andeunscx)
print(sum(andeunscx^2))
tandeunscx<-microbenchmark(andeunscx2<-nlsalt::numericDeriv(rexpr, theta, rho=weedenv, eps=1e-10))
print(tandeunscx)

## Central diff with smaller eps
andeunscx2<-nlsalt::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE, eps=1e-10)
print(andeunscx2)
print(sum(andeunscx2^2))
tandeunscx2<-microbenchmark(andeunscx2<-nlsalt::numericDeriv(rexpr, theta, rho=weedenv, central=TRUE, eps=1e-10))
print(tandeunscx2)
```

The `dir` parameter allows us to use a backward difference for the derivative. This 
appears in `nlsModel()` for the case where a parameter is on an upper bound for
the case `algorithm="port"`. It does not check for nearness to the bound, and 
for the lower bound assumes that we are stepping AWAY from the bound in the
default direction (`dir=+1`). None of the code addresses the issue where bounds
are closer together than the step used for the finite difference, so there are
situations where we could crash the code. Nor does the code check if the central
difference is specified when near a bound. 

- In the case of lower bounds, a central difference can overstep the bound when
  a parameter is "close" or on the bound.
- In the case of an upper bound, changing the `dir` will not change the derivative
  expression and steps in both forward and backward directions of the parameter
  are taken.
  
  

## Symbolic methods from `nlsr`

The package `nlsr` has a function `model2rjfun()` that converts an expression
describing how the residual functions are computed into an R function that
computes the residuals at a particular set of parameters and sets the 
**attribute** "gradient" of the vector of residual values to the Jacobian at
the particular set of parameters. 

```{r nlsr1}
# nlsr has function model2rjfun. We can evaluate just the residuals
res0<-model2rjfun(eunsc, start1, data=weeddata1, jacobian=FALSE)
res0(start1)
# or the residuals and jacobian
## nlsr::model2rjfun forms a function with gradient (jacobian) attribute
funsc <- model2rjfun(eunsc, start1, data=weeddata1) # from nlsr: creates a function
tmodel2rjfun <- microbenchmark(model2rjfun(eunsc, start1, data=weeddata1))
print(tmodel2rjfun)
print(funsc)
print(funsc(start1))
print(environment(funsc))
print(ls.str(environment(funsc)))
print(ls(environment(funsc)$data))
eval(eunsc, environment(funsc))
vfunsc<-funsc(start1)
print(vfunsc)
tfunsc<-microbenchmark(funsc(start1))
print(tfunsc)
```

# `numDeriv` package

The package `numDeriv` includes a function `jacobian()` that acts on a user
function `resid()` to produce the Jacobian at a set of parameters by several
choices of approximation. 

```{r numDeriv1}
# We use the residual function (without gradient attribute) from nlsr
jeunsc<-jacobian(res0, start1)
jeunsc
# Timings of the analytic jacobian calculations
tjeunsc<-microbenchmark(jeunsc<-jacobian(res0, start1))
print(tjeunsc)
```

Note that the manual pages for `numDeriv` offer many options for the functions in
the package. At 2021-5-27 we have yet to explore these.

# Comparisons

In the following, we are comparing to `vfunsc`, which is the evaluated 
residual vector at `start1=c(1,1,1)` with "gradient" attribute (jacobian)
included, as developed using package `nlsr`. This is taken as the "correct"
result, even though it is possible that the generated order of calculations
may introduce inaccuracies in the supposedly analytic derivatives.

`numericDeriv` computes a similar structure (residuals with "gradient" attribute):
`ndeunsc`: the forward difference result with default `eps` (1e-07 according to manual)
`ndeunsc2`: Central difference with default `eps`
`ndeunscx`: Forward difference with smaller eps=1e-10
`ndeunscx2`: Central difference with smaller eps=1e-10

`jeunsc`: numDeriv::jacobian() result with default settings.

```{r compjac1}
## Matrix comparisons
attr(ndeunsc, "gradient")-attr(vfunsc,"gradient")
attr(ndeunsc2, "gradient")-attr(vfunsc,"gradient")
attr(ndeunscx, "gradient")-attr(vfunsc,"gradient")
attr(ndeunscx2, "gradient")-attr(vfunsc,"gradient")
jeunsc-attr(vfunsc,"gradient")

## Summary comparisons
max(abs(attr(ndeunsc, "gradient")-attr(vfunsc,"gradient")))
max(abs(attr(ndeunsc2, "gradient")-attr(vfunsc,"gradient")))
max(abs(attr(ndeunscx, "gradient")-attr(vfunsc,"gradient")))
max(abs(attr(ndeunscx2, "gradient")-attr(vfunsc,"gradient")))
max(abs(jeunsc-attr(vfunsc,"gradient")))
```


# Performance results for different computing environments

Here we present tables of the results, preceded by identified descriptions of the machines we
used. We use ideas and functions from the document `MachineSummary` to provide a characterization
and identity for each machine used.

M21-LM20.1 

?? still to be run

?? What machines provide a range of possibilities.

# Discussion of derivative computation for nonlinear least squares

In no particular order, we comment on some issues relating to the Jacobian calculations
in nonlinear least squares.

## Nomenclature

R is not in step with many other areas of numerical computation when labelling different
objects in the nonlinear least squares problem. In particular, R uses the term "gradient"
when the object of interest is the Jacobian matrix. In that it is useful in performing 
iterations of the Gauss-Newton or related equations to have the Jacobian associated with
the residuals, and the rows of the Jacobian matrix are "gradients" of the respective
residuals, we can accept the attribute name "gradient" to select the required information.
Moreover, as in package `nlsr` it is very useful to have the Jacobian matrix as an 
attribute of the residual vector, since the main solver function, in this case `nlsr::nlfb()`,
can be called with the same input for the arguments `res` and `jac`. These are the functions
required to compute the residual and the Jacobian, and using the same function for both is
very convenient, but needs some way to return both the residual vector and Jacobian matrix
in a coherent fashion. 

## Numerical approximation near constraints

As far as we are aware there is no software that implements a fully safeguarded system to
compute numerical approximation of the Jacobian (or gradients in general optimization) near
constraints. The same statement applies even in the case of the much simpler bounds constraints.
Users have a perverse tendency to devise ways to foil our best efforts. For example, they may
decide that a good way to specify fixed (i.e., masked) parameters that they do not want to vary
during a particular calculation is to specify the lower and upper bound of a parameter at the
same value. Later runs may want the parameter constraints relaxed. 

In `nlsr::nlxb()`, users may, in fact, specify masked parameters this way. This is a case of 
"if you can't beat them, join them", but it does provide an easily understood way for users to
fix values. 

More tricky is dealing with constraints that are close together. Note that these may arise from,
for example, two linear (planar) constraints that approach at a narrow angle. In the apex where
these constraints intersect, we will have tight bounds on parameters. If the constraint is not
one that is imposed by the nature of the residual or objective function, for example, a log() 
or square root near zero, then we can generally proceed and allow the derivative approximator
to evaluate outside the constraints. Things are decidedly nastier if we do have inadmissible
values. 

The issue of constraints and the need for a step in parameter values for derivative approximations
was one of the motivations for trying to find analytic derivatives in package `nlsr` and the 
continuing effort to bring them into other R tools.


# Appendix 1: Base R numericDeriv code

This code is in two files, nls.R and nls.c and is extracted here.

## From nls.R

```{r nlsR, comment=NA, echo=FALSE}
   cat(readLines('./nlsND.R'), sep = '\n')
```


## From nls.c

```{r nlsc, comment=NA, echo=FALSE}
   cat(readLines('./nlsND.c'), sep = '\n')
```

# Appendix 2: numericDeriv() from nlsalt package (all in R)

```{r nlsndnew, comment=NA, echo=FALSE}
   cat(readLines('./nlsalt/R/nlsnd.R'), sep = '\n')
```
