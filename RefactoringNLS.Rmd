---
title: "Refactoring the nls() function in R"
author: 
   - John C Nash, retired professor, University of Ottawa
   - Arkajyoti Bhattacharjee, Indian Institute of Technology, Kanpur
date: "2021-7-17"
output: 
    pdf_document:
        keep_tex: false
        toc: true
bibliography: ImproveNLS.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## require(bookdown) # language engine to display text - does not seem necessary
```

# Abstract

This article reports the particular activities of our Google Summer of Code
project "Improvements to nls()" that relate to R code for that function, which
is intended for the estimation of models written as a formula that has at least
one parameter that is not estimable via solving a set of linear equations. A
companion document "Variety in Nonlinear Least Squares Codes" presents an 
overview of methods for the problem which takes a much wider view of the
problem of minimizing a function that can be written as a sum of squared
terms.

Our work has not fully addressed all the issues that we would like to see
resolved, but we believe we have made sufficient progress to demonstrate that
there are worthwhile improvements that can be made to the R function `nls()`.
An important overall consideration in our work has been the maintainability of
the code base that supports the `nls()` functionality, as we believe that the
existing code makes maintenance and improvement very difficult.

# The existing nls() function: strenghts and shortcomings

`nls()` is the tool in base R (the distributed software package from https://cran.r-project.org)
for estimating nonlinear statistical models. The function was developed mainly in the 1980s and
1990s by Doug Bates et al., initially for S (https://en.wikipedia.org/wiki/S_%28programming_language%29). 
The ideas spring primarily from the book by @bateswatts.

The `nls()` function has a remarkable and quite comprehensive set of capabilities for estimating
nonlinear models that are expressed as formulas. In particular, we note that it
- handles formulas that include R functions
- allows data to be subset
- permits parameters to be indexed over a set of related data
- produces measures of variability (i.e., standard error estimates) for the estimated parameters
- has related profiling capabilities for exploring the likelihood surface as parameters are changed

With such a range of features and a long history, it is not surprising that code has become untidy 
and overly patched. It is, to
our mind, essentially unmaintainable. Moreover, its underlying methods can and should be improved. Let
us review some of the issues. We will then propose corrective actions, some of which we have carried out.

## Issue: Convergence and termination tests

Within the standard documentation (**man**ual or ".Rd" file) `nls()` warns

>  **The default settings of nls generally fail on artificial “zero-residual” data problems.**

>  The nls function uses a relative-offset convergence criterion that compares the numerical imprecision at
  the current parameter estimates to the residual sum-of-squares. This performs well on data of the form

>  $$y = f(x, \theta) + eps$$

>  (with var(eps) > 0). It fails to indicate convergence on data of the form

>  $$y = f(x, \theta)$$

>  because the criterion amounts to comparing two components of the round-off error. To avoid a zero-divide in
  computing the convergence testing value, a positive constant scaleOffset should be added to the denominator
  sum-of-squares; it is set in control; this does not yet apply to algorithm = "port".

It turns out that this issue can be quite easily resolved. The key "convergence test" -- more properly
a "termination test" for the **program** rather than testing for convergence of the underlying **algorithm** --
is the Relative Offset Convergence Criterion (see @BatesWatts81). This works by projecting the proposed
step in the parameter vector on the gradient and estimating how much the sum of squares loss function 
will decrease. To avoid scale issues, we use the current size of the loss function as a measure and divide
by it. When we have "converged", the estimated
decrease is very small, as usually is its ratio to the sum of squares. However, in some cases we have the 
possibility of an exact fit and the sum of squares is (almost) zero and we get the possibility of a 
zero-divide failure. 

The issue is easily resolved by adding a small quantity to the loss function. To preserve legacy behaviour,
in 2021, one of us (JN) proposed that `nls.control()` have an additional parameter `scaleOffset` with a
default value of zero for legacy behaviour. Setting it to a small number -- 1.0 is a reasonable choice --
allows small-residual problems (i.e., near-exact fits) to be dealt with easily. We call this the
**safeguarded relative offset convergence criterion**.

We are pleased to report that this improvement is in the R-devel distributed code at time of writing
and will migrate to the base R distribution when updated.

### Example of a small-residual problem

```
rm(list=ls())
t <- -10:10
y <- 100/(1+.1*exp(-0.51*t))
lform<-y~a/(1+b*exp(-c*t))
ldata<-data.frame(t=t, y=y)
plot(t,y)
lstartbad<-c(a=1, b=1, c=1)
lstart2<-c(a=100, b=10, c=1)
nlsr::nlxb(lform, data=ldata, start=lstart2)
nls(lform, data=ldata, start=lstart2, trace=TRUE)
# Fix with scaleOffset
nls(lform, data=ldata, start=lstart2, trace=TRUE, control=list(scaleOffset=1.0))
sessionInfo()
```

Edited output of running this function follows:

```
> rm(list=ls())
> t <- -10:10
> y <- 100/(1+.1*exp(-0.51*t))
> lform<-y~a/(1+b*exp(-c*t))
> ldata<-data.frame(t=t, y=y)
> plot(t,y)
> lstart2<-c(a=100, b=10, c=1)
> nlsr::nlxb(lform, data=ldata, start=lstart2)
nlsr object: x 
residual sumsquares =  1.007e-19  on  21 observations
    after  13    Jacobian and  19 function evaluations
  name            coeff          SE       tstat      pval      gradient    JSingval   
a                    100     2.679e-11  3.732e+12  1.863e-216  -6.425e-11       626.6  
b                    0.1      3.78e-13  2.646e+11  9.125e-196  -3.393e-08       112.3  
c                   0.51       6.9e-13  7.391e+11  8.494e-204   1.503e-08       2.791  
# Note that this has succeeded. The test in nlsr recognizes small residual problems.
> nls(lform, data=ldata, start=lstart2, trace=TRUE)
40346.    (1.08e+00): par = (100 10 1)
11622.    (2.93e+00): par = (101.47 0.49449 0.71685)
5638.0    (1.08e+01): par = (102.23 0.38062 0.52792)
642.08    (1.04e+01): par = (102.16 0.22422 0.41935)
97.712    (1.79e+01): par = (100.7 0.14774 0.45239)
22.250    (1.78e+02): par = (99.803 0.093868 0.50492)
0.025789  (1.33e+03): par = (100.01 0.10017 0.50916)
6.0571e-08 (7.96e+05): par = (100 0.1 0.51)
4.7017e-19 (1.86e+04): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
1.2440e-27 (5.71e-01): par = (100 0.1 0.51)
Error in nls(lform, data = ldata, start = lstart2, trace = TRUE) : 
  number of iterations exceeded maximum of 50

> nls(lform, data=ldata, start=lstart2, trace=TRUE, control=list(scaleOffset=1.0))
40346.      (1.08e+00): par = (100 10 1)
11622.      (2.91e+00): par = (101.47 0.49449 0.71685)
5638.0      (9.23e+00): par = (102.23 0.38062 0.52792)
642.08      (5.17e+00): par = (102.16 0.22422 0.41935)
97.712      (2.31e+00): par = (100.7 0.14774 0.45239)
22.250      (1.11e+00): par = (99.803 0.093868 0.50492)
0.025789    (3.79e-02): par = (100.01 0.10017 0.50916)
6.0571e-08  (5.80e-05): par = (100 0.1 0.51)
4.7017e-19  (1.62e-10): par = (100 0.1 0.51)
Nonlinear regression model
  model: y ~ a/(1 + b * exp(-c * t))
   data: ldata
     a      b      c 
100.00   0.10   0.51 
 residual sum-of-squares: 4.7e-19

Number of iterations to convergence: 8 
Achieved convergence tolerance: 1.62e-10

> sessionInfo()
R version 4.1.0 (2021-05-18)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 20.2
```

### More general termination tests

The single convergence criterion of `nls()` leaves out some possibilities that could
be useful for some problems. The package `nlsr` (@nlsr-manual) already offers both 
the safeguarded relative offset test (**roffset**) as well as a **small sum of 
squares** test (**smallsstest**) that compares the latest evaluated sum of squared
(weighted) residuals to a very small multiple of the initial sum of squares. The
multiple uses a control setting `offset` which defaults to 100.0 and we compute
the 4th power of the machine epsilon times this offset.

```{r meps4, echo=TRUE}
epstol<-100*.Machine$double.eps
e4 <- epstol^4
e4
```

We do note that `nls()` stops after `maxiter` "iterations". However, for almost all
iterative algorithms, the meaning of "iteration" requires careful examination of the
code. Instead, we prefer to record the number of times the residuals or the jacobian have
been computed and put upper limits on these. Our codes exit (terminate) when these limits
are reached. Generally we prefer larger limits than the default `maxiter=50` of `nls()`,
but that may simply reflect our history of dealing with more difficult problems as we are
the tool-makers users consult when things go wrong. 


## Issue: Failure when Jacobian is computationally singular

This is the infamous "singular gradient" termination. A Google search of 

```
R nls "singular gradient"
```

gets over 4000 hits that are spread over the years. 
In some cases this is due
to failure of the simple finite difference approximation of the Jacobian in the
`numericDeriv()` function that is a part of `nls()`. `nlsr` can use analytic
derivatives, and we can import this functionality to the `nls()` code as an
improvement. See below in the section **Jacobian computation**.

However, the more common source of the issue is that the Jacobian is very close
to singular for some values of the model parameters. In such cases we need to find
an alternative algorithm to the Gauss-Newton iteration of `nls()`. The most common
work-around is the Levenberg-Marquardt stabilization (@Marquardt1963, Levenberg1944, 
jn77ima). Versions of this have been implemented in packages `minpack.lm` and `nlsr`.
and we have preliminary versions of an `nls` replacement that can incorporate a
version of the Levenberg-Marquardt stabilization. (There are some issues of 
integration with other code structures and of complexity of the computations that
suggest we should use a simplified LM stabilization.)


## Issue: Jacobian computation

`nls()`, with the `numericDeriv()` function, computes the Jacobian as the "gradient"
attribute of the residual vector. This is implemented as a mix of R and C code, but
we have created a rather more compact version entirely in R in this Google Summer of
Code project. See the document **DerivsNLS.pdf**.

```{r code=xfun::read_utf8('Tests/Examples/badJlogmod.R')}
```

## Issue: Subsetting

`nls()` accepts an argument `subset`. Unfortunately, this acts through the mediation of
`model.frame` and is not clearly obvious in the source code files `/src/library/stats/R/nls.R` and 
`/src/library/stats/src/nls.C`. 

- implementation via weights
- implementation via model.frame
- other concerns

## Issue: na.action

`na.action` is an argument to the `nls()` function, but it does not appear in obviously in the
source code ...

## Issue: model frame

`model` is an argument to the `nls()` function, but it does not appear obviously in the
source code ...

## Issue: documentation of results

We have noticed that there are some important issues relating to the documentation of residuals,
fits, and related objects computed by R modeling functions.

The functions `resid()` (an alias for `residuals()`) and `fitted()` and `lhs()` are UNWEIGHTED.
But if we return `ans` from `nls()` or `minpack.lm::nlsLM` or our new `nlsj` (interim package), 
then `ans$m$resid()` is WEIGHTED.

## Issue: sources of data

`nls()` can be called without specifying the `data` argument. In this case, it will
search in the available environments (i.e., workspaces) for suitable data objects. 
We do NOT like this approach. R allows users to leave many objects in the default
(.GlobalEnv) workspace. Moreover, users have to actively suppress saving this 
workspace (`.RData`) on exit, and any such file in the path when R is launched will
be loaded. 

Nevertheless, to provide compatible behaviour with `nls()`, we will need to ensure
that equivalent behaviour is guaranteed.


## Issue: missing start vector and self-starting models

Nonlinear estimation algorithms are almost all iterative and need a set of starting
parameters. `nls()` offers a special class of modeling formulae called **selfStart** 
models. There are a number of these in base R (see list below) and others in R packages
such as CRAN package nlraa (@MiguezNLRAA2021). Unfortunately, the structure of the programming of 
these is such that the methods by which initial parameters are computed is entangled
with the particularities of the `nls()` code. Though there is a `getInitial()` function,
this is not easy to use to simply compute the initial parameter estimates.

?? TODO: find a way to use getInitial() more easily.

### selfStart models in base R

```
SSasymp
SSasympOff
SSasympOrig
SSbiexp
SSfol
SSfpl
SSlogis
SSmicmen
SSgompertz2
SSweibull
```

?? weird output in testing

<!-- ?? https://www.r-bloggers.com/2020/02/a-collection-of-self-starters-for-nonlinear-regression-in-r/ -->
<!-- may be useful -->

```
> ls()
[1] "ldata"     "lform"     "lstart2"   "lstartbad" "t"         "y"        
> apar <- getInitial(y~SSlogis(t, Asym, xmid, scal), data=ldata)
Error in nls(y ~ 1/(1 + exp((xmid - x)/scal)), data = xy, start = list(xmid = aux[[1L]],  : 
  number of iterations exceeded maximum of 50
```

In the event that a selfStart model is not available, `nls()` sets all the starting parameters 
to 1. This is, in our view, tolerable, but could possibly be improved by using a set of values
that are slightly different e.g., in the case of a model 

$$  y \tilde a*exp(-b*x) + c*exp(-d*x)$$

it would be useful to have $b$ and $d$ values different so the Jacobian is not singular. Thus
some sort of sequence like  1.0, 1.1, 1.2, 1.3 for the four parameters might be better and it
can be provided quite simply instead of all 1s. 

## Issue: documentation of the results of running nls()

The output of nls() is an object of class "nls" which has the following structure:

?? put in an example and document it.

### Concerns with content of the nls object

The nls object contains some elements that are awkward to produce by other algorithms.
Moreover, some information that would be useful is not presented obviously (??examples -
convergence/termination info, Jsingvals)

## Issue: code structure

The `nls()` code is structured in a way that inhibits both maintenance and improvement.
In particular, the iterative setup is such that introduction of Marquardt stabilization
is not easily available. 

To obtain performance, a lot of the code is in C with consequent calls and returns that
complicate the code. Over time, R has become much more efficient on modern computers, and
the need to use compiled C and Fortran is less critical. Moreover, the burden for maintenance
could be much reduced by moving code entirely to R.

## Issue: code documentation for maintenance 

`setPars()` -- explain weaknesses. Only used by profile.nls()

The paucity of documentation is exacerbated by the mixed R/C/Fortran code base.

Following is an email to Dr. Heather Turner from John Nash.

```
I'm afraid that I don't know the purpose of the recursive call either.  I know that I wrote the code to use a closure
for the response, covariates, etc., but I don't recall anything like a recursive call being necessary.

If the R sources were in a git repository I might try to use `git blame` to find out when and by whom that was written
but they are in an SVN repository, I think, and I haven't used it for a long, long time.

I don't think I will be of much help.  My R skills have atrophied to the point where I wouldn't even know how to start
exploring what is happening in the first call as opposed to the recursive call.


On Tue, Jun 29, 2021 at 11:50 AM John Nash <Nashjc@uottawa.ca <mailto:Nashjc@uottawa.ca>> wrote:

    Thanks.

    https://gitlab.com/nashjc/improvenls/-/blob/master/Croucher-expandednlsnoc.R
    <https://gitlab.com/nashjc/improvenls/-/blob/master/Croucher-expandednlsnoc.R>

    This has the test problem and the expanded code. Around line 367 is where we are
    scratching our heads. The function code (from nlsModel()) is in the commented lines below
    the call. This is

          # > setPars
          # function(newPars) {
          #   setPars(newPars)
          #   resid <<- .swts * (lhs - (rhs <<- getRHS())) # envir = thisEnv {2 x}
          #   dev   <<- sum(resid^2) # envir = thisEnv
          #   if(length(gr <- attr(rhs, "gradient")) == 1L) gr <- c(gr)
          #   QR <<- qr(.swts * gr) # envir = thisEnv
          #   (QR$rank < min(dim(QR$qr))) # to catch the singular gradient matrix
          # }

    I'm anticipating that we will be able to set up a (possibly inefficient) code
    with documentation that will be easier to follow and test, then gradually figure
    out how to make it more efficient.

    The equivalent from minpack.lm is

    setPars = function(newPars) {
                setPars(newPars)
                assign("resid", .swts * (lhs - assign("rhs", getRHS(),
                    envir = thisEnv)), envir = thisEnv)
                assign("dev", sum(resid^2), envir = thisEnv)
                assign("QR", qr(.swts * attr(rhs, "gradient")), envir = thisEnv)
                return(QR$rank < min(dim(QR$qr)))
            }


    In both there is the recursive call, which must have a purpose I don't understand.

    Cheers, JN

    On 2021-06-29 12:33 p.m., Douglas Bates wrote:
    > *Attention : courriel externe | external email*
    > Thanks for contacting me, John.  Can you point me to a file in the gitlab.com <http://gitlab.com>
    <http://gitlab.com <http://gitlab.com>> repository that
    > contains the definition of setPars?
    >
    > (By the way, it is probably best to use the email address dmbates@gmail.com <mailto:dmbates@gmail.com>
    <mailto:dmbates@gmail.com <mailto:dmbates@gmail.com>> for me.  If email
    > goes to bates@stat.wisc.edu <mailto:bates@stat.wisc.edu> <mailto:bates@stat.wisc.edu <mailto:bates@stat.wisc.edu>>
    it should get forwarded to the gmail.com <http://gmail.com> <http://gmail.com <http://gmail.com>>
    > address but sometimes gmail decides that such mail looks suspicious and puts it in the spam folder.  Not sure
    why.  For
    > a long time I used bates@stat.wisc.edu <mailto:bates@stat.wisc.edu> <mailto:bates@stat.wisc.edu
    <mailto:bates@stat.wisc.edu>> as my "From:" address because it had been my address
    > since the 80's but even gmail got suspicious of mail from that address that did not appear to originate in the
    wisc.edu <http://wisc.edu>
    > <http://wisc.edu <http://wisc.edu>> domain.)
    >
```

# Goals of our effort

Here are some of the goals we hope to accomplish.

## Code rationalization and documentation

We want

- to provide a packaged version of `nls()` (call it `nlsalt`) coded entirely in R 
  that matches the version in base R or what is packaged in `nlspkg` as described 
  in the "PkgFromRbase" document.
  
- try to obtain a cleaner structure for the overall `nls()` infrastructure. By this we
  mean a re-factoring of the routines so they are better suited to maintenance of both
  the existing `nls()` methods and features as well as the new features we would like
  to add.

- try to explain what we do, either in comments or separate maintainer documentation.
  Since we are complaining about the lack of explanatory material for the current
  code, we feel it incumbent on us to provide such material for our own work, and if
  possible for the existing code.

## Provide tests

We need suitable tests in order:

- to ensure our new `nlsalt` or related packages work properly, in particular, giving
  results comparable to or better than the `nls()` in base R or `nlspkg`;
- to test individual solver functions to ensure they work across the range of calling 
  mechanisms, that is, different ways of supplying inputs to the solver(s);
- to pose "silly" inputs to nonlinear least squares solvers (in R) to see if 
  these bad input exceptions are caught by the programs.

### A test runner program

When we have a "new" or trial solver function, we would like to know if it gives 
acceptable results on a range of sample problems of different types, starting
parameters, input conditions, constraints, subsets, weights or other settings. 
Ideally we want to be able to get a summary that is easy to read and assess. For
example, one approach would be to list the names of a set of tests with a red, green
or yellow dot beside the name for FAILURE, SUCCESS, or "NOT APPLICABLE". In the last
category would be a problem with constraints that the solver is not designed to 
handle. 

To accomplish this, we need a suitable "runner" program that can be supplied with
the name of a solver or solvers and a list of test problem cases. Problems generally
have a base setup -- a specification of the function to fit as a formula, some data
and a default starting set of parameters. Other cases can be created by imposing
bounds or mask constraints, subsets of the data, and different starts. 

How to set up this "runner" and its supporting infrastructure is non-trivial. While
the pieces are not as complicated as the inter-related parts of the solvers, especially
`nls()`, the categorization of tests, their documentation, and the structuring to make
running them straightforward and easy requires much attention to detail.

Some questions ???

- do we need a "base" script for each family of test problem, with numbered particular
  cases?
  
- how should we document the families and cases? What tags do we need to allow us to
  quickly select lists of tests
  
- structure to output the results. AB's draft csv file and runner program.??


??? this is for the quick testing of sets of problems --- 
documented in TestsDoc.

# Output of the project

?? see Working Doc etc.

- formal reports
- informal reports
- problem sets
- code and documentation


# References
